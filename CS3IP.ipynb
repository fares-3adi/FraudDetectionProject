{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fares-3adi/FraudDetectionProject/blob/main/CS3IP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4FZ2lRItZji"
      },
      "source": [
        "# **Credit Card Fraud Detection Using Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVhZwFiLIteE"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SnOoj8o6GH3c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd1ugoIdJeTr"
      },
      "source": [
        "## Dataset and Acknowledgements\n",
        "\n",
        "The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
        "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "Link to Dataset: https://www.openml.org/search?type=data&status=active&id=42175\n",
        "\n",
        "Citations:\n",
        "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
        "\n",
        "Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
        "\n",
        "Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
        "\n",
        "Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
        "\n",
        "Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Ael; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
        "\n",
        "Carcillo, Fabrizio; Le Borgne, Yann-Ael; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n",
        "\n",
        "Bertrand Lebichot, Yann-Ael Le Borgne, Liyun He, Frederic Oble, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n",
        "\n",
        "Fabrizio Carcillo, Yann-Ael Le Borgne, Olivier Caelen, Frederic Oble, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "1aqq_HZv6mUa",
        "outputId": "e0dd1515-680e-4252-a968-f99a43fd1e52"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-164dfe96ae0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Loading the dataset directly from google drive to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# not fully loading it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Loading the dataset directly from google drive to avoid\n",
        "# not fully loading it\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab/CS3IP/creditcard.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-PWTcIuKkkT"
      },
      "source": [
        "For reference: The column names are Time, V1 ... V28, Amount, and Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AAwfLehPCbs"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQSHzgsWNF1s"
      },
      "source": [
        "# **Exploratory Data Analysis**\n",
        "\n",
        "In this section, we examined the transaction data to understand feature distributions, detect anomalies, and assess class imbalance. Insights from this analysis directly informed decisions such as transformations, feature selection, sampling strategies and so on in preprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwe3MqZIKEDQ"
      },
      "source": [
        "## **Histogram/KDE Plots**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPfG8xi5K-kh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# function to calculate percentage overlap\n",
        "def calculate_overlap(hist1, hist2, bin_widths):\n",
        "  overlap_area = np.sum(np.minimum(hist1, hist2) * bin_widths)\n",
        "  return overlap_area * 100  # Convert to percentage\n",
        "\n",
        "# plotting histograms with KDE overlays\n",
        "num_cols = len(df.columns) - 1\n",
        "num_rows = (num_cols + 3) // 4\n",
        "\n",
        "plt.figure(figsize=(16, 4 * num_rows))\n",
        "\n",
        "for i, label in enumerate(df.columns[:-1], 1):  # exclude the class column\n",
        "  plt.subplot(num_rows, 4, i)\n",
        "\n",
        "  # create histograms\n",
        "  fraud_data, bins = np.histogram(df[df[\"Class\"] == 1][label], bins=30, density=True)\n",
        "  non_fraud_data, _ = np.histogram(df[df[\"Class\"] == 0][label], bins=bins, density=True)\n",
        "\n",
        "  # calculate bin widths\n",
        "  bin_widths = np.diff(bins)\n",
        "\n",
        "  # calculate overlap\n",
        "  overlap_percentage = calculate_overlap(fraud_data, non_fraud_data, bin_widths)\n",
        "\n",
        "  # plot histograms\n",
        "  plt.hist(df[df[\"Class\"] == 1][label], bins=bins, color=\"red\", alpha=0.5, density=True, label=\"Fraud\")\n",
        "  plt.hist(df[df[\"Class\"] == 0][label], bins=bins, color=\"blue\", alpha=0.5, density=True, label=\"Non-Fraud\")\n",
        "\n",
        "  # add KDE lines to the same plot\n",
        "  sns.kdeplot(df[df[\"Class\"] == 1][label], color=\"red\", label=\"Fraud KDE\", linestyle=\"dashed\")\n",
        "  sns.kdeplot(df[df[\"Class\"] == 0][label], color=\"blue\", label=\"Non-Fraud KDE\", linestyle=\"dashed\")\n",
        "\n",
        "  # add title with overlap percentage\n",
        "  plt.title(f\"{label} (Overlap: {overlap_percentage:.2f}%)\")\n",
        "  plt.ylabel(\"Probability\")\n",
        "  plt.xlabel(label)\n",
        "  plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZDJP25HWDW5"
      },
      "source": [
        "### Analyzing Overlap for Feature Selection\n",
        "\n",
        "**Note**: The overlap percentage displayed on top of each histogram shows the percentage by which the two classes overlap for that given feature. A lower percentage overlap indicates a greater divide between the two classes given a classification based on that feature.\n",
        "\n",
        "**The aim here is to look for the following categories of features:**\n",
        "- Features with an overlap of over 60% would indicate low discriminative power and would therefore be dopped.\n",
        "- Features with an overlap under 40% would indicate high discriminative power and would therefore be retained.\n",
        "- Features with an overlap in the middle range of 40-60% would require further analysis in order to determine whether they should be dropped or retained.\n",
        "\n",
        "**Observing the results:**\n",
        "\n",
        "Features with an overlap of 40% or below are (To Be Retained):\n",
        "- V2 (39.88%)\n",
        "- V3 (30.74%)\n",
        "- V11 (26.35%)\n",
        "- V12 (22.50%)\n",
        "- V14 (16.20%)\n",
        "- V16 (30.06%)\n",
        "- V17 (21.20%)\n",
        "- V27 (41.66%)\n",
        "\n",
        "\n",
        "Features with an overlap above 60% (To Be Dropped):\n",
        "- V8 (66.75%)\n",
        "- V15 (89.62%)\n",
        "- V21 (90.58%)\n",
        "- V22 (85.60%)\n",
        "- V24 (79.95%)\n",
        "- V25 (85.60%)\n",
        "- V26 (84.30%)\n",
        "\n",
        "Features with an overlap of 40-60%, needing further analysis (Using Random Forest):\n",
        "- V1 (54.20%)\n",
        "- V6 (51.37%)\n",
        "- V9 (42.77%)\n",
        "- V28 (50.24%)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QYPanryOZbN"
      },
      "source": [
        "## **Random Forest Feature Importance for All Dataset Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8j-9VoxOpcV"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define Features (excluding the class column)\n",
        "X = df.drop(columns=['Class'])\n",
        "y = df['Class']\n",
        "\n",
        "# train a random forest model\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced')\n",
        "rf.fit(X, y)\n",
        "\n",
        "# get feature importances\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# print and visualize feature importances\n",
        "print(\"Feature Importances:\\n\", feature_importances)\n",
        "\n",
        "# plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "feature_importances.plot(kind='bar', title=\"Feature Importance by Random Forest\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.xticks(rotation=90)  # rotating to make it horziontal\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, average_precision_score\n",
        "\n",
        "# seprating features (x) and target (y)\n",
        "X = df.drop(columns=['Class'])\n",
        "y = df['Class']\n",
        "\n",
        "# defining parameter grid for random forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# using average precision as a performance metric for imbalanced data\n",
        "avg_precision_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
        "\n",
        "# creating a random forest model with class weighting for imbalance\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "\n",
        "# setting up cross-validation and GridSearch\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(\n",
        "    rf,\n",
        "    param_grid=param_grid,\n",
        "    scoring=avg_precision_scorer,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# running the grid search\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# retrieve and print best parameterss and best cv score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best CV Score (Average Precision):\", grid_search.best_score_)\n",
        "\n",
        "# getting the best model and extracting feature importances\n",
        "best_rf = grid_search.best_estimator_\n",
        "feature_importances = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print(\"\\nFeature Importances from Best RF Model:\\n\", feature_importances)\n",
        "\n",
        "# plotting the feature importnces\n",
        "plt.figure(figsize=(10, 6))\n",
        "feature_importances.plot(kind='bar', title=\"Feature Importance (Tuned Random Forest)\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VKp4EyTtSyTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4KayZZ3P1qF"
      },
      "source": [
        "Based on the feature importance scores shown in the above plot, it can be seen that features such as V14, V10 and V12 possess high importance.\n",
        "\n",
        "This means they hold high predictive power when it comes to model performance. Whereas, features such as V24, V25 and V15, ones with a low feature importnace score, hold low predictive power.\n",
        "\n",
        "Therefore, we are likely to see better performance, if those features are dropped (ones with low predictive score)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKinnJ-K7w0j"
      },
      "source": [
        "## **Feature Importance with SHAP**\n",
        "\n",
        "- To explain how SHAP plots work, the x-axis represents the SHAP value, which indicates how much a feature pushes the model's prediction towards fraud (positive SHAP values) or non-fraud (negative SHAP values)\n",
        "\n",
        "  - Points that are further to the right (positive SHAP values) mean the feature contributes to predicting fraud (Class = 1).\n",
        "\n",
        "  - Points that are further to the left (negative SHAP values) mean the feature contributes to predicting non-fraud (Class = 0).\n",
        "\n",
        "- The color gradient represents the features values where:\n",
        "\n",
        "  - Blue represents low features values.\n",
        "  - Red represents high feature values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, average_precision_score\n",
        "\n",
        "# separating features and target\n",
        "X = df.drop(columns=[\"Class\"])\n",
        "y = df[\"Class\"]\n",
        "\n",
        "# splitting the data into training (80%) and test (20%) sets while preserving class distribution\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# defining a small parameter grid for tuning XGBoost\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100],\n",
        "    \"max_depth\": [3, 6],\n",
        "    \"learning_rate\": [0.1, 0.01],\n",
        "    \"subsample\": [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# using average precision as the scoring metric, appropriate for imbalanced data\n",
        "avg_prec_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
        "\n",
        "# setting up cross-validation with StratifiedKFold\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# initializing the XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
        "\n",
        "# running GridSearchCV to tune hyper-parameters on the training set\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring=avg_prec_scorer,\n",
        "                           cv=cv, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best CV Score (Average Precision):\", grid_search.best_score_)\n",
        "\n",
        "# using the best model obtained from tuning for SHAP analysis on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# creating a SHAP Explainer and compute SHAP values on the test set\n",
        "explainer = shap.Explainer(best_model)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# plotting a SHAP summary of the feature importances\n",
        "shap.summary_plot(shap_values, X_test, max_display=len(X_test.columns)-1)\n"
      ],
      "metadata": {
        "id": "s8I348tOT_3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9JnliyJ_Kvo"
      },
      "source": [
        "**V14 is the Most Important Feature**\n",
        "- V14 has the highest impact on model output, meaning it plays the most significant role in predicting fraud.\n",
        "\n",
        "- Higher absolute SHAP values indicate stronger influence on the model's decision.\n",
        "\n",
        "- The color gradiant shows that low values of V14 (blue) push predictions towards fraud (right), while high values (red) increase non-fraud probability (left).\n",
        "\n",
        "**Other Key Features (V4, V12, V10) Also Have High Importance**\n",
        "\n",
        "- V4, V12, and V10 follow V14 in importance, meaning they contribute strongly to separating between fraud and non-fraud.\n",
        "\n",
        "- Features like Amount and Time appear mid-range, suggesting they play a moderate role but are not as dominant as PCA-transformed variables.\n",
        "\n",
        "- Amount has a balanced spread of SHAP values, meaning its contribution varies between fraud and non-fraud cases.\n",
        "\n",
        "**Features Lower in the List Have Little to No Impact**\n",
        "\n",
        "- Features like V28, V18, and V6 HAVE much smaller SHAP values, meaning they contribute less to fraud classification.\n",
        "\n",
        "- This suggests these features could be considered for removal.\n",
        "\n",
        "**One thing worth considering is the following**\n",
        "\n",
        "- The models struggle at predicting the fraud class more than anything.\n",
        "\n",
        "- From the plot it can be notices that there are some features that possess low importance yet deviate more towards the fraud class.\n",
        "\n",
        "- Features V17, V20, V22 and V13 in particular exhibit this (in that order of deviation).\n",
        "\n",
        "- Therefore, it is concluded that these features should be retained.\n",
        "\n",
        "**Describing Each Feature**\n",
        "\n",
        "- V14\n",
        "  - Blue (low) → Right (fraud), Red (high) → Left (non-fraud).\n",
        " - Low V14 increases fraud probability, high V14 predicts non-fraud.\n",
        "- V4\n",
        "  - Blue (low) → Left (Non-fraud), Red (high) → Right (fraud).\n",
        "  - High V4 increases likelihood of fraud.\n",
        "- V17\n",
        "  - Blue (low) → Right (fraud), with some Red (high) on right side.\n",
        "  - Low V17 values associated with fraud.\n",
        "- V10\n",
        "  - Red (high) → Left (non-fraud), Blue (low) → Right (fraud).\n",
        "  - Low V10 increases fraud likelihood.\n",
        "- V7\n",
        "  - Red (high values) on both sides.\n",
        "  - Weak decider between fraud and non-fraud class.\n",
        "- V11\n",
        "  - Red (high) → Right (fraud), Blue (low) → Left (non-fraud).\n",
        "  - Higher V11 values tied to fraud.\n",
        "- V20\n",
        "  - Mostly Blue (low) on the right side (fraud).\n",
        "  - Low V20 values lean more towards fraud.\n",
        "- V12\n",
        "  - Red (high) → Left (non-fraud), Blue (low) → Right (fraud).\n",
        "  - Low V12 pushes towards fraud prediction.\n",
        "- V19\n",
        "  - Blue (low) → Right (fraud).\n",
        "  - Low V19 linked to fraud.\n",
        "- V21\n",
        "  - Red (high) → Right (fraud), Blue (low) → Left (non-fraud).\n",
        "  - Though somewhat jumbled, somewhat weak decider.\n",
        "- V13\n",
        "  - Mixed distribution, no clear pattern\n",
        "- Amount\n",
        "  - Mixed distribution, no clear pattern\n",
        "- V27\n",
        "  - Red (high) → Left (non-fraud)\n",
        "  - High V27 leans towards non-fraud\n",
        "- V22\n",
        "  - Red (high) → Right (fraud)\n",
        "  - High V22 leans towards fraud\n",
        "-  V16\n",
        "  - Somewhat mixed up, but mostly Blue (low) → Right (fraud), Red (high) → Left (non-fraud)\n",
        "  - Mediocre decider between fraud and non-fraud.\n",
        "- V26\n",
        "  - Blue (low) on both sides with a mix of red and blue on the left side (non-fraud).\n",
        "  - Weak decider between fraud and non-fraud.\n",
        "- V24\n",
        "  - Mixed distribution.\n",
        "  - Low effect, unclear pattern, weak decider.\n",
        "\n",
        "- Features V2, V9, V6, V8, V15, V28, V3, V18, Time, V5, V23, V1, V25\n",
        "  - All these features have no-clear distribution on the SHAP plot, however\n",
        "\n",
        "- Based on this as well as the results from the Feature Importance Scores using RandomForestClassifier, the following actionable is derived:\n",
        "\n",
        "  - Derived Actionable: Drop Features V27, V1, V26, V13, V28, V6, V5, V15, V25, V24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgDpOfmC2vJb"
      },
      "source": [
        "## **Correlation Heatmap to Identify Redundant Features**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN4OaAdU2zK5"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df.corr(), cmap=\"coolwarm\", annot=False, vmin=-1, vmax=1)\n",
        "plt.title(\"Correlation Heatmap of Features\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdku8yCs4_-h"
      },
      "source": [
        "- Since the dataset is PCA-transformed we see a light gray background which suggests that most features do not exhibit strong correlation with each other (No Correlation).\n",
        "\n",
        "- Some PCA features show weak correlations (shown by faint blue and orange spots)\n",
        "\n",
        "- The Time and Amount columns seem to show minor correlations with a few PCA features but nothing extreme.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgG9ozjEQ6hs"
      },
      "source": [
        "Based on this, we can derive the following actionable step:\n",
        "\n",
        "**-> Derived Actionable: Drop the following features: V27, V1, V20, V8, V26, V13, V28, V23, V22, V6, V5, V15, V25, Time and V24**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucPmO6IAmFmo"
      },
      "source": [
        "## **Outlier Detection and Handling (For Time and Amount Column)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOQQs1T645sc"
      },
      "source": [
        "## Box Plots & Interquartile Method for Outlier Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di_tQ15aOty6"
      },
      "outputs": [],
      "source": [
        "# outlier detection by plotting box plotss\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x='Class', y='Amount', data=df)\n",
        "plt.title(\"Boxplot of Amount by Class\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x='Class', y='Time', data=df)\n",
        "plt.title(\"Boxplot of Time by Class\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlVm0clOvo7M"
      },
      "source": [
        "**The Amount Column:**\n",
        "\n",
        "The boxplot shows a large number of outliers in the Amount Column for both classes, particularly for the non-fraud class. This could also be discerned from observing the Amount column's histogram which was highly skewed.\n",
        "\n",
        "Since the Amount column is highly skewed with extreme values, a log transformation should be applied.\n",
        "\n",
        "**The Time Column:**\n",
        "\n",
        "The Time Column does not show any significant difference between fraud and non-fraud transactions, it might not contribute much to classification, therefore it should likely be dropped.\n",
        "\n",
        "If not, it can be normalized to ensure consistent scaling.\n",
        "\n",
        "**-> Derived Actionable: Apply log transformation to Amount Column (After that the Amount column is Normalized).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8x_Ir6F47RI"
      },
      "source": [
        "## Interquartile Method (IQR) For Outlier Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxD3FW5A4_uP"
      },
      "outputs": [],
      "source": [
        "# detecting outliers using IQR\n",
        "for column in ['Amount', 'Time']:  # Analyze 'Amount' and 'Time'\n",
        "  Q1 = df[column].quantile(0.25)\n",
        "  Q3 = df[column].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "  lower_bound = Q1 - 1.5 * IQR\n",
        "  upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "  # Define the 'outliers' variable for the current column\n",
        "  outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "\n",
        "  # Check fraud cases among outliers\n",
        "  fraud_outliers = outliers[outliers['Class'] == 1]\n",
        "  print(f\"Outliers in {column}: {len(outliers)}\")\n",
        "  print(f\"Fraud cases among outliers in {column}: {len(fraud_outliers)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO5P9QHC1d7J"
      },
      "source": [
        "This output shows 31904 rows flagged as outliers in the Amount column based on the IQR method.\n",
        "\n",
        "These outliers are transactions with amounts significantly higher than most other transactions (e.g. exceeding the upper whisker in the boxplot above)\n",
        "\n",
        "\n",
        "For the time column, there is not outlier-specific action needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqAlLgCEK_vO"
      },
      "source": [
        "## Visualizing the Class Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMX0gByX-5R_"
      },
      "outputs": [],
      "source": [
        "# visualizing the split between fraud and non-fraud class\n",
        "class_num = pd.value_counts(df[\"Class\"], sort=True)\n",
        "class_num.plot(kind=\"bar\", rot=0)\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# checking the percentage of fraud and non-fraud instances\n",
        "print(f\"Percentage of Fraud (Class==1): {df['Class'].value_counts()[0]/len(df) * 100}%\")\n",
        "print(f\"Percentage of Non-Fraud (Class==0): {df['Class'].value_counts()[1]/len(df) * 100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwgLH6Q6CTHZ"
      },
      "source": [
        "The bar chart above shows that fraud class accounts for 492 cases out of the total 284,807 transactions (Indicating a major class imbalance)\n",
        "\n",
        "Only 0.173% of the dataset contains cases of fraud, while 99.827% is not fraud (values may vary to a few d.p). This means the dataset is heavily skewed/imbalanced and that will need to be dealt with.\n",
        "\n",
        "**-> Derived Actionable: Oversample the Fraud class and/or undersample the Non-fraud class. Possibly use SMOTE.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da03OiEACQJx"
      },
      "outputs": [],
      "source": [
        "# compute statistical summary\n",
        "statistical_summary = df.describe()\n",
        "\n",
        "# display the summary\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.float_format\", \"{:.2f}\".format)  # Optional: Format decimal places\n",
        "print(statistical_summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. Overall descriptive statistics (same as df.describe())\n",
        "# ------------------------------------------------------------------\n",
        "overall_summary = df.describe()\n",
        "print(\"\\n===== Overall Summary (all rows) =====\")\n",
        "print(overall_summary)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2. Descriptive statistics *separately* for each class\n",
        "#    (count, mean, std, min, median, max)\n",
        "# ------------------------------------------------------------------\n",
        "summary_by_class = (\n",
        "    df.groupby(\"Class\")\n",
        "      .agg([\"count\", \"mean\", \"std\", \"min\", \"median\", \"max\"])\n",
        ")\n",
        "print(\"\\n===== Summary by Class (0 = non‑fraud, 1 = fraud) =====\")\n",
        "print(summary_by_class)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3. Skewness and kurtosis for every feature\n",
        "#    (helpful for spotting heavy tails and asymmetry)\n",
        "# ------------------------------------------------------------------\n",
        "skew_kurt = pd.DataFrame({\n",
        "    \"skew\": df.skew(),\n",
        "    \"kurtosis\": df.kurt()\n",
        "})\n",
        "print(\"\\n===== Skewness and Kurtosis =====\")\n",
        "print(skew_kurt)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4. Quick check for missing values\n",
        "# ------------------------------------------------------------------\n",
        "missing = df.isnull().sum()\n",
        "print(\"\\n===== Missing‑Value Count per Column =====\")\n",
        "print(missing[missing > 0] if missing.sum() else \"No missing values found.\")\n"
      ],
      "metadata": {
        "id": "EAA88756VNJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPZeh8jxC-Mi"
      },
      "source": [
        "PCA components (V1 - V28)\n",
        "- Already centred and scaled by the transformation, so no extra standardisation is needed.\n",
        "- A few components show heavy tails, so robust models remain a sensible choice.\n",
        "\n",
        "Amount\n",
        "- Strongly right-skewed: most transactions are small, but a handful are very large.\n",
        "- High spread confirms why a log transformation will be applied.\n",
        "\n",
        "Time\n",
        "- Spans the full two-day window of the dataset and sits on a much larger scale than the PCA features.\n",
        "- Should be z-standardised (or possibly dropped) to keep the feature set on a comparable scale.\n",
        "\n",
        "Class imbalance\n",
        "- Fraud cases make up a tiny fraction of the data, reinforcing the need for resampling or class-weighted learning and for metrics such as precision-recall AUC.\n",
        "\n",
        "Data completeness\n",
        "- No missing values were detected, so no imputation step is required.\n",
        "\n",
        "Fraud vs non-fraud contrasts\n",
        "- Fraud transactions tend to occur slightly earlier in the time window and involve somewhat higher amounts.\n",
        "- Several PCA features show different averages between the two classes, indicating useful separation.\n",
        "\n",
        "**-> Derived Actionable: Log transform Amount Column and Standardize Time Column**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrRPu5nmOb6-"
      },
      "source": [
        "## Transaction Amount Analysis\n",
        "\n",
        "Exploring if there are distinct patterns in Amount for fraud and non-fraud transactions by visualizing and calculating summary statistics.\n",
        "\n",
        "For example, fraud might occur more often with lower or higher amounts.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ojkQHZcOe-U"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Pull Amount values directly from the original DataFrame\n",
        "fraud_amounts     = df.loc[df[\"Class\"] == 1, \"Amount\"]\n",
        "nonfraud_amounts  = df.loc[df[\"Class\"] == 0, \"Amount\"]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.kdeplot(fraud_amounts,    label=\"Fraud\",     fill=True)   # 'fill' replaces 'shade' in recent seaborn\n",
        "sns.kdeplot(nonfraud_amounts, label=\"Non-Fraud\", fill=True)\n",
        "\n",
        "plt.title(\"Transaction Amounts (Fraud vs. Non-Fraud)\")\n",
        "plt.xlabel(\"Amount\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idiaU-KKTILf"
      },
      "source": [
        "*  Both Fraud and Non-fraud transaction amounts are heavily skewed toward smaller values.\n",
        "\n",
        "* Fraud transactions (blue line) appear concentrated around lower values (close to 0), while Non-Fraud transactions (orange line) has a broader spread into larger amounts but still concentrated near 0.\n",
        "\n",
        "  *  There are extreme values, especially for Non-Fraud transactions, where the curve extends towards 25,000.\n",
        "\n",
        "* There is significant overlap between Fraud and Non-Fraud classes at lower transaction amounts, which aligns with the summary statistics and earlier histogram analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn-D64krYVda"
      },
      "source": [
        "## Explained Variance Analysis for PCA Transformed Features\n",
        "\n",
        "Since the features V1 to V28 are already PCA-transformed and the explained variance ratios were not saved or provided during the PCA process, there is no direct way to calculate the explained variance because the original PCA model and data are not available.\n",
        "\n",
        "The alternatives such as measuring feature importance of the fraud and non-fraud overlap for each feature were measured above to compensate for the lack of analysis on explained variance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5e--yPdBR8X"
      },
      "source": [
        "## Summing Up Actionable Steps for Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF_kTIsYmnoB"
      },
      "source": [
        "### Actionable Steps for Data Preprocessing (Ordered)\n",
        "\n",
        "*  **Actionable Step 1:** Drop Features V8, V15, V21, V22, V24, V25, V26 **(Feature Selection)**\n",
        "\n",
        "*  *Intermediate Step: Split dataset into Train, Validation & Test Sets.*\n",
        "\n",
        "*  **Actionable Step 2:** Apply log transformation to Amount Column **(Outlier Handling)**\n",
        "\n",
        "*  **Actionable Step 3**: Normalize the Amount and Time column **(Scaling Features for consistent ranges)**\n",
        "\n",
        "*  **Actionable Step 4:** Oversample the Fraud class and/or undersample the Non-fraud class. Possible use SMOTE.\n",
        "\n",
        "Questions:\n",
        "\n",
        "1. Do all of these data preprocessing steps only apply to the training set or does it at some point apply to the other sets (test and validation) and if it does, when?\n",
        "\n",
        "2. What is the difference between standardization and normalization and which one is more applicble for PCA transformed for the Amount column (Given other features are PCA transformed)?\n",
        "\n",
        "3.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1elBI68xqLI_"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM5tPscrvrNG"
      },
      "source": [
        "In this step we will apply the data preprocessing actionable steps derived from our data analysis above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYe1Zon2M0bm"
      },
      "source": [
        "### Actionable Step 1 - Drop Irrelvant Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gI4F4zDNKcQ"
      },
      "outputs": [],
      "source": [
        "# Drop irrelevant features\n",
        "''' df = df.drop(columns=['V27', 'V1', 'V20', 'V8', 'V26', 'V13', 'V28', 'V23', 'V22',\n",
        "                      'V6', 'V5', 'V15', 'V25', 'Time', 'V24']) '''\n",
        "\n",
        "'''df = df.drop(columns=['V27', 'V1', 'V26', 'V13', 'V28', 'V6', 'V5', 'V15', 'V25', 'V24', 'Time']) '''\n",
        "\n",
        "df = df.drop(columns=['V27', 'V1', 'V26', 'V13', 'V28', 'V6', 'V5', 'V15', 'V25', 'V24'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKf6YVU7tFKk"
      },
      "source": [
        "### Actionable Step #2 - Splitting Dataset into training, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae-ZCfSnwHTn"
      },
      "outputs": [],
      "source": [
        "# Splitting data set into training, validation and testing\n",
        "# 0-70%: Training\n",
        "# 70-100% of Dataset: Testing\n",
        "training_set, testing_set = train_test_split(df, test_size=0.3, random_state=42, stratify=df['Class'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcGJUvvNNUAM"
      },
      "source": [
        "### Actionable Step #3 - Log Transform the Amount Column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxIoIJVnNW2K"
      },
      "outputs": [],
      "source": [
        "# Log transformation for Amount column\n",
        "for subset in [training_set, testing_set]:\n",
        "    subset['Amount'] = np.log1p(subset['Amount'])  # Use log1p to handle zeros safely\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIhhhdeUBRvK"
      },
      "outputs": [],
      "source": [
        "# Compute statistical summary for the trainin set\n",
        "statistical_summary = training_set.describe()\n",
        "\n",
        "# Display the summary\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.float_format\", \"{:.2f}\".format)  # Optional: Format decimal places\n",
        "print(statistical_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMGOmDz1A0gF"
      },
      "source": [
        "#**K-Fold Cross Validation & Hyper Parameter Tuning**\n",
        "\n",
        "K-Fold Cross Validation is performed at this stage as the first 3 preprocessing steps preceding it are:\n",
        "\n",
        "- Dropping Irrelevant Features\n",
        "- Creating the Train-Test Split\n",
        "- Log Transformations of the Amount Column\n",
        "\n",
        "\n",
        "K-Fold CV has to take place before the training set undergoes Normalization and SMOTE, as these preprocessing steps need to take place inside the cross-validation loop. This is such that each fold's training portion has its own preprocessing applied without knowledge of the validation portion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sv1d1-tSilV"
      },
      "source": [
        "### Moving Forward:\n",
        "\n",
        "- Leave the dataset as is and use ensemble methods with class weight balanced. Testing on stratified k-fold. Do this without any resampling.\n",
        "\n",
        "- Research methods to deal with imbalanced datasets.\n",
        "\n",
        "- Double-check model for any issues with data pre-processing. Re-evaluate your dara analysis and data preprocessing.\n",
        "\n",
        "### What is missing:\n",
        "\n",
        "- Results, comments on results,\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbavSkivB0wj"
      },
      "source": [
        "## Setting up the Train and Test Sets for K-Fold Cross Validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZP2bCPnB5Fe"
      },
      "outputs": [],
      "source": [
        "# Define features/targets (DO NOT SCALE OR OVERSAMPLE YET)\n",
        "X_train = training_set.drop(columns=['Class'])\n",
        "y_train = training_set['Class']\n",
        "X_test = testing_set.drop(columns=['Class'])\n",
        "y_test = testing_set['Class']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_t8I1nLGpSu"
      },
      "source": [
        "## Imports Needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08fwiJwkGseS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, make_scorer, precision_score, recall_score, f1_score\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, classification_report\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOpyOH5IJ-0K"
      },
      "source": [
        "## 1. Defining Hyper-Parameters for each Model\n",
        "\n",
        "This defines the search space for hyper-parameter tuning for each learning model.\n",
        "\n",
        "- Naive Bayes has no tunable hyper-parameters, so we leave it as an empty dictionary.\n",
        "\n",
        "- Logistic Regression: We tune:\n",
        "  - *c:* which controls regularization strength\n",
        "  - *max-iter*: which defines the maximum number of iterations for convergence.\n",
        "\n",
        "- Random Forest: We tune multiple parameters such as:\n",
        "  - *n_estimators:* The number of trees in the forest\n",
        "  - *max-depth:* The maximum depth of each tree\n",
        "  - *min_samples_leaf:* The minimum number of samples required in a leaf node.\n",
        "\n",
        "- Support Vector Machines (SVM): We tune:\n",
        "  - *c:* Controls regularization\n",
        "  - *kernel:* Specifies the kernel type (linear or RBF)\n",
        "  - *gamma:* Controls the influence of a single training example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfUDMjREEytR"
      },
      "source": [
        "- look into kernel hyper parameter for naive bayes\n",
        "- break down the procedure, do not too much at once\n",
        "- separate models, run hyper-parameter tuning for each model on its own\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4tftQvWFkFG"
      },
      "source": [
        "## 2. Defining the Models and Scoring Metrics\n",
        "\n",
        "This initializes the models that will be trained and tested.\n",
        "\n",
        "- GaussianNB(): Naive Bayes classifier (no hyper-parameters to tune).\n",
        "- LogisticRegression(): Includes class_weight='balanced' to handle class imbalance.\n",
        "- RandomForestClassifier: Uses class_weight='balanced' to account for fraud cases.\n",
        "\n",
        "Here as well, evaluation metrics for model performance are defined:\n",
        "\n",
        "- Precision: Measures how many predicted fraud cases are actually fraud.\n",
        "- Recall: Measures how many actual fraud cases were corretly detected.\n",
        "- F1-score: Balances precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0axnjjFA3yr"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Naive Bayes\": GaussianNB(var_smoothing=1e-3),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
        "    \"Support Vector Machines\": SVC(kernel='linear', class_weight='balanced', random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(class_weight='balanced', random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42),\n",
        "    \"Balanced Random Forest\": BalancedRandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=3,  # Adjusted\n",
        "    class_weight=\"balanced_subsample\",  # More dynamic weighting\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        "),\n",
        "    \"LightGBM\": LGBMClassifier(objective=\"binary\", random_state=42)\n",
        "}\n",
        "\n",
        "# Define Scoring Metrics for GridSearchCV\n",
        "scoring = {\n",
        "    \"Precision\": make_scorer(precision_score, pos_label=1),\n",
        "    \"Recall\": make_scorer(recall_score, pos_label=1),\n",
        "    \"F1-Score\": make_scorer(f1_score, pos_label=1),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98vf2vKFFxoi"
      },
      "source": [
        "## 3. Defining Pipelines for Each Model\n",
        "\n",
        "- ColumnTransformer ensures that only the \"Amount\" and \"Time\" columns are standardized, leaving PCA-transforme features unchanges.\n",
        "\n",
        "- Pipeline are defined to steamline training:\n",
        "  - scaler: Standardizes \"Amount\" and \"Time\"\n",
        "  - smote: Applies SMOTE inside cross-validation to balanmce fraud and non-fraud cases.\n",
        "  - classifier: The machine learning model itself.\n",
        "\n",
        "- Stratified K-Fold: is used for cross-validation to ensure each fold maintains the class imbalance distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKrVg-8WGmwz"
      },
      "source": [
        "### Alternative Code: Applies ADASYN to only the training folds in the cross-validation - try and compare results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf7LEZbPInzu"
      },
      "source": [
        "The combination of oversampling and undersampling in partcular needs to be applied within the training folds of the cross validation rather than to the entire training set prior.\n",
        "\n",
        "This is in order to prevent any form of data leakage, as during cross validation, a part of the training set is used for validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W23mc6UhGtLV"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn import FunctionSampler\n",
        "\n",
        "# --- Hybrid Sampling Function ---\n",
        "def apply_hybrid_sampling(X, y):\n",
        "  adasyn = ADASYN(sampling_strategy=0.2, random_state=42)\n",
        "  rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
        "\n",
        "  X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
        "  X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "  return X_resampled, y_resampled\n",
        "\n",
        "hybrid_sampler = FunctionSampler(func=apply_hybrid_sampling, validate=False)\n",
        "\n",
        "# FunctionSampler to apply the hybrid approach inside the cross-validation\n",
        "hybrid_sampler = FunctionSampler(func=apply_hybrid_sampling, validate=False)\n",
        "\n",
        "# Define Standard Scaler for 'Amount' column only\n",
        "scaler = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scaler', StandardScaler(), ['Amount', 'Time'])  # Scale only 'Amount'\n",
        "    ], remainder='passthrough'  # Keep PCA features unchanged\n",
        ")\n",
        "\n",
        "# Define Pipelines for Each Model\n",
        "pipelines = {}\n",
        "for model_name, model in models.items():\n",
        "  pipelines[model_name] = ImbPipeline([\n",
        "    (\"hybrid_sampling\", hybrid_sampler),\n",
        "    (\"scaler\", scaler),\n",
        "    (\"classifier\", model)\n",
        "  ])\n",
        "\n",
        "# Initialize Stratified K-Fold\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jatbkTVKM5Py"
      },
      "source": [
        "## 4. Performing Hyper-Parameter Tuning using GridSearchCV\n",
        "\n",
        "Here GridSearchCV is used for Hyper-parameter tuning to find the best parameters for each model.\n",
        "\n",
        "- If a model has hyper-parameters to tune, GridSearchCV will:\n",
        "\n",
        "  - Test all hyper-parameter combinations\n",
        "  - Train and evaluate each combination using 5-Fold Stratified Cross Validation.\n",
        "  - Select the best performing combination based on F1-score.\n",
        "\n",
        "- If a model doesn't have hyper-parameters (e.g. Naive Bayes), it is trained directly without tuning.\n",
        "\n",
        "- After tuning, the best models are evaluated on the test set, and results are diplayed using a classification report.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMDDJrsPLJTv"
      },
      "source": [
        "- XGBoost\n",
        "- BalancedRandomForest\n",
        "- LightGBM\n",
        "- Ensemble Systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA6hMPpzziYU"
      },
      "outputs": [],
      "source": [
        "best_models = {}\n",
        "best_params = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvuAHP4rxu8W"
      },
      "source": [
        "### Naive Bayes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbRhEuttx0Nw"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid for Naive Bayes\n",
        "nb_param_grid = {\n",
        "    \"classifier__var_smoothing\": [1e-9, 1e-6, 1e-3, 1e-1]\n",
        "}\n",
        "\n",
        "# Run GridSearchCV for Naive Bayes\n",
        "grid_search_nb = GridSearchCV(\n",
        "    pipelines[\"Naive Bayes\"],\n",
        "    nb_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search_nb.fit(X_train, y_train)\n",
        "best_models[\"Naive Bayes\"] = grid_search_nb.best_estimator_\n",
        "best_params[\"Naive Bayes\"] = grid_search_nb.best_params_\n",
        "\n",
        "print(f\"Best Parameters for Naive Bayes: {grid_search_nb.best_params_}\\n\")\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "y_pred_nb = best_models[\"Naive Bayes\"].predict(X_test)\n",
        "print(\"\\nNaive Bayes Performance on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_nb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDJ95a39x0w_"
      },
      "source": [
        "### Logistic Regression (Hyper-parameter Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUm6EYBEx3nO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print(\"Running GridSearchCV for Logistic Regression...\\n\")\n",
        "\n",
        "# Parameters for Logistic Regression\n",
        "lr_param_grid = {\n",
        "    \"classifier__C\": [0.01, 0.1, 1, 10],  # Regularization strength\n",
        "    \"classifier__max_iter\": [100, 500, 1000],\n",
        "    \"classifier__solver\": [\"liblinear\", \"saga\"]  # Added for better coverage\n",
        "}\n",
        "\n",
        "lr_pipeline = pipelines[\"Logistic Regression\"]\n",
        "grid_search_lr = GridSearchCV(\n",
        "    lr_pipeline,\n",
        "    lr_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"f1\",  # Using F1-score as the primary metric\n",
        "    n_jobs=-1,\n",
        "    verbose=1  # Show progress\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search_lr.fit(X_train, y_train)\n",
        "\n",
        "# Store the best model and parameters\n",
        "best_models[\"Logistic Regression\"] = grid_search_lr.best_estimator_\n",
        "best_params[\"Logistic Regression\"] = grid_search_lr.best_params_\n",
        "\n",
        "print(f\"Best Parameters for Logistic Regression: {grid_search_lr.best_params_}\\n\")\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "y_pred_lr = best_models[\"Logistic Regression\"].predict(X_test)\n",
        "print(\"\\nLogistic Regression Performance on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_lr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTgicAeSx5TD"
      },
      "source": [
        "### Random Forest (Hyper-parameter Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYyCZnY_x8EQ"
      },
      "outputs": [],
      "source": [
        "print(\"Running GridSearchCV for Random Forest...\\n\")\n",
        "\n",
        "rf_param_grid = {\n",
        "    \"classifier__n_estimators\": [100, 200],\n",
        "    \"classifier__max_depth\":    [15, None],\n",
        "    \"classifier__min_samples_split\": [2, 5],\n",
        "    \"classifier__min_samples_leaf\":  [1, 2],\n",
        "    \"classifier__max_features\": [\"sqrt\"]\n",
        "}\n",
        "\n",
        "rf_pipeline = pipelines[\"Random Forest\"]\n",
        "grid_search_rf = GridSearchCV(\n",
        "    rf_pipeline,\n",
        "    rf_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Store the best model and parameters\n",
        "best_models[\"Random Forest\"] = grid_search_rf.best_estimator_\n",
        "best_params[\"Random Forest\"] = grid_search_rf.best_params_\n",
        "\n",
        "print(f\"Best Parameters for Random Forest: {grid_search_rf.best_params_}\\n\")\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "y_pred_rf = best_models[\"Random Forest\"].predict(X_test)\n",
        "print(\"\\nRandom Forest Performance on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRsIbkYwyBEh"
      },
      "source": [
        "### Support Vector Machines (Hyper-parameter Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8W1u2wXyJiU"
      },
      "outputs": [],
      "source": [
        "print(\"Running GridSearchCV for Support Vector Machines...\\n\")\n",
        "\n",
        "svm_param_grid = {\n",
        "    \"classifier__C\": [0.1, 1, 10, 100],\n",
        "    \"classifier__kernel\": [\"linear\", \"rbf\"],\n",
        "    \"classifier__gamma\": [\"scale\", \"auto\"],\n",
        "    \"classifier__class_weight\": [None, \"balanced\"]  # Added exploration\n",
        "}\n",
        "\n",
        "svm_pipeline = pipelines[\"Support Vector Machines\"]\n",
        "grid_search_svm = GridSearchCV(\n",
        "    svm_pipeline,\n",
        "    svm_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "\n",
        "# Store the best model and parameters\n",
        "best_models[\"Support Vector Machines\"] = grid_search_svm.best_estimator_\n",
        "best_params[\"Support Vector Machines\"] = grid_search_svm.best_params_\n",
        "\n",
        "print(f\"Best Parameters for Support Vector Machines: {grid_search_svm.best_params_}\\n\")\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "y_pred_svm = best_models[\"Support Vector Machines\"].predict(X_test)\n",
        "print(\"\\nSupport Vector Machines Performance on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puMyA6aWvbW_"
      },
      "source": [
        "### XGBoost (Hyper-parameter Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArSw2HRovg8i"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Define the parameter grid for XGBoost\n",
        "xgb_param_grid = {\n",
        "    \"classifier__n_estimators\": [50, 100, 200],  # Number of trees\n",
        "    \"classifier__max_depth\": [3, 6, 9],  # Depth of trees\n",
        "    \"classifier__learning_rate\": [0.01, 0.1, 0.2],  # Step size shrinkage\n",
        "    \"classifier__subsample\": [0.8, 1.0],  # Fraction of samples used for training each tree\n",
        "    \"classifier__colsample_bytree\": [0.8, 1.0]  # Fraction of features used for each tree\n",
        "}\n",
        "\n",
        "# Define XGBoost Pipeline\n",
        "pipelines[\"XGBoost\"] = ImbPipeline([\n",
        "    (\"scaler\", scaler),  # Apply scaling to Amount (already defined)\n",
        "    (\"hybrid_sampling\", hybrid_sampler),  # Apply ADASYN + Undersampling\n",
        "    (\"classifier\", models[\"XGBoost\"])  # XGBoost model\n",
        "])\n",
        "\n",
        "# Run GridSearchCV for XGBoost\n",
        "grid_search_xgb = GridSearchCV(\n",
        "    pipelines[\"XGBoost\"],\n",
        "    xgb_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "best_models[\"XGBoost\"] = grid_search_xgb.best_estimator_\n",
        "best_params[\"XGBoost\"] = grid_search_xgb.best_params_\n",
        "\n",
        "print(f\"Best Parameters for XGBoost: {grid_search_xgb.best_params_}\\n\")\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "y_pred_xgb = best_models[\"XGBoost\"].predict(X_test)\n",
        "print(\"\\nXGBoost Performance on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# --- Precision-Recall Curve ---\n",
        "# Get predicted probabilities for the positive class (fraud)\n",
        "y_scores_xgb = best_models[\"XGBoost\"].predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve and average precision score\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores_xgb)\n",
        "avg_precision = average_precision_score(y_test, y_scores_xgb)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}', color='darkorange')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for XGBoost')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pyMUI9gJV3X"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKUasgvosBDW"
      },
      "source": [
        "### LightGBM (Hyper-parameter Tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFtCADXH5tlM"
      },
      "source": [
        "### Alternative LightGBM Plot with Precision-Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBmOhfq75xb2"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "\n",
        "print(\"Running GridSearchCV for LightGBM...\\n\")\n",
        "\n",
        "# Define the hyperparameter grid for LightGBM\n",
        "lgbm_param_grid = {\n",
        "    \"classifier__n_estimators\": [100, 200],  # Fewer iterations, but enough for convergence\n",
        "    \"classifier__max_depth\": [-1],  # -1 allows automatic depth selection\n",
        "    \"classifier__learning_rate\": [0.05, 0.1],  # Keeps it small but not too slow\n",
        "    \"classifier__num_leaves\": [31, 50],  # Default setting (optimized for speed)\n",
        "    \"classifier__boosting_type\": [\"gbdt\"],  # Only use GBDT (fastest and best generalization)\n",
        "    \"classifier__subsample\": [1.0],  # Use full data for training (avoid unnecessary complexity)\n",
        "    \"classifier__colsample_bytree\": [1.0],  # Use all features in each boosting round\n",
        "    \"classifier__scale_pos_weight\": [10, 25, 50, 100]  # Helps balance fraud class\n",
        "}\n",
        "\n",
        "# Define LightGBM Pipeline\n",
        "pipelines[\"LightGBM\"] = ImbPipeline([\n",
        "    (\"scaler\", scaler),  # Apply scaling to Amount (already defined)\n",
        "    (\"hybrid_sampling\", hybrid_sampler),  # Apply ADASYN + Undersampling\n",
        "    (\"classifier\", models[\"LightGBM\"])  # LightGBM model\n",
        "])\n",
        "\n",
        "# Run GridSearchCV for LightGBM\n",
        "grid_search_lgbm = GridSearchCV(\n",
        "    pipelines[\"LightGBM\"],\n",
        "    lgbm_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search_lgbm.fit(X_train, y_train)\n",
        "\n",
        "# Store the best model and parameters\n",
        "best_models[\"LightGBM\"] = grid_search_lgbm.best_estimator_\n",
        "best_params[\"LightGBM\"] = grid_search_lgbm.best_params_\n",
        "\n",
        "print(f\"Best Parameters for LightGBM: {grid_search_lgbm.best_params_}\\n\")\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "y_pred_lgbm = best_models[\"LightGBM\"].predict(X_test)\n",
        "print(\"\\nLightGBM Performance on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_lgbm))\n",
        "\n",
        "# --- Precision-Recall Curve ---\n",
        "# Get predicted probabilities for the positive class (fraud)\n",
        "y_scores_lgbm = best_models[\"LightGBM\"].predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve and average precision score\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores_lgbm)\n",
        "avg_precision = average_precision_score(y_test, y_scores_lgbm)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}', color='purple')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for LightGBM')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVeWg3wRXPN7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05TkveYWJgZM"
      },
      "source": [
        "### BalancedRandomForest (Hyper-parameter Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7m9xjaxJrii"
      },
      "outputs": [],
      "source": [
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "\n",
        "# Define the parameter grid for Balanced Random Forest\n",
        "brf_param_grid = {\n",
        "    \"classifier__n_estimators\": [100, 200],  # Reduce options\n",
        "    \"classifier__max_depth\": [10, 20],  # Reduce depth choices\n",
        "    \"classifier__min_samples_split\": [5, 10],  # Reduce split options\n",
        "    \"classifier__min_samples_leaf\": [1, 2],  # Reduce leaf node sizes\n",
        "    \"classifier__bootstrap\": [True]  # Fix bootstrap to one value\n",
        "}\n",
        "\n",
        "# Define Balanced Random Forest Pipeline\n",
        "pipelines[\"Balanced Random Forest\"] = ImbPipeline([\n",
        "    (\"scaler\", scaler),  # Apply scaling to Amount (already defined)\n",
        "    (\"hybrid_sampling\", hybrid_sampler),  # Apply ADASYN + Undersampling\n",
        "    (\"classifier\", models[\"Balanced Random Forest\"])  # Balanced Random Forest model\n",
        "])\n",
        "\n",
        "# Run GridSearchCV for Balanced Random Forest\n",
        "grid_search_brf = GridSearchCV(\n",
        "    pipelines[\"Balanced Random Forest\"],\n",
        "    brf_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search_brf.fit(X_train, y_train)\n",
        "best_models[\"Balanced Random Forest\"] = grid_search_brf.best_estimator_\n",
        "best_params[\"Balanced Random Forest\"] = grid_search_brf.best_params_\n",
        "\n",
        "print(f\"Best Parameters for Balanced Random Forest: {grid_search_brf.best_params_}\\n\")\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "y_pred_brf = best_models[\"Balanced Random Forest\"].predict(X_test)\n",
        "print(\"\\nBalanced Random Forest Performance on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_brf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7fCDdtoO7ip"
      },
      "outputs": [],
      "source": [
        "sum(y_train_balanced == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V39L4eqDPHWl"
      },
      "outputs": [],
      "source": [
        "sum(y_train_balanced == 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA6QzdMrPSQA"
      },
      "source": [
        "As we can see above, the number of 0 and 1 classes in the training dataset is equal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BPWTNwbQqxu"
      },
      "source": [
        "### Defining Training and Testing Features and Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CiNR8bfQqH5"
      },
      "outputs": [],
      "source": [
        "# 1. Separate features and target for the balanced training set\n",
        "X_train = training_set_balanced.drop(columns=['Class'])\n",
        "y_train = training_set_balanced['Class']\n",
        "\n",
        "# 2. Prepare test set (features and target)\n",
        "X_test = testing_set.drop(columns=['Class'])\n",
        "y_test = testing_set['Class']\n",
        "\n",
        "# Verify dimensions\n",
        "print(f\"Training set: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n",
        "print(f\"Testing set: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_models)"
      ],
      "metadata": {
        "id": "bLXMUjsknzyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_params)"
      ],
      "metadata": {
        "id": "H87fnSLo6tzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBH2OstN8ejB"
      },
      "source": [
        "# Final Run for all Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes\n"
      ],
      "metadata": {
        "id": "Dh5lQ_AY6T0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Final Test Evaluation: Naïve Bayes ===\n",
        "model_name = \"Naive Bayes\"\n",
        "model = best_models[model_name]\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(f\"\\n=== {model_name} — Test-set performance ===\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "ap = average_precision_score(y_test, y_scores)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f\"AP = {ap:.3f}\", color=\"purple\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(f\"Precision–Recall Curve — {model_name}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PB4VH88P6Vw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "dLCr0ckP6Xuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Final Test Evaluation: Logistic Regression ===\n",
        "model_name = \"Logistic Regression\"\n",
        "model = best_models[model_name]\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(f\"\\n=== {model_name} — Test-set performance ===\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "ap = average_precision_score(y_test, y_scores)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f\"AP = {ap:.3f}\", color=\"purple\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(f\"Precision–Recall Curve — {model_name}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S2Z5sHgl6Zj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "_3ewkvhk6bi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Final Test Evaluation: Support Vector Machine ===\n",
        "model_name = \"Support Vector Machines\"\n",
        "model = best_models[model_name]\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_scores = model.decision_function(X_test)  # for SVM, use decision_function\n",
        "\n",
        "print(f\"\\n=== {model_name} — Test-set performance ===\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "ap = average_precision_score(y_test, y_scores)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f\"AP = {ap:.3f}\", color=\"purple\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(f\"Precision–Recall Curve — {model_name}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zvb2pXAK6eEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "I5YibAIT6gNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Final Test Evaluation: Random Forest ===\n",
        "model_name = \"Random Forest\"\n",
        "model = best_models[model_name]\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(f\"\\n=== {model_name} — Test-set performance ===\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "ap = average_precision_score(y_test, y_scores)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f\"AP = {ap:.3f}\", color=\"purple\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(f\"Precision–Recall Curve — {model_name}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uGvB_GHX6fff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "jGNcmcxP6jqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Final Test Evaluation: XGBoost ===\n",
        "model_name = \"XGBoost\"\n",
        "model = best_models[model_name]\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(f\"\\n=== {model_name} — Test-set performance ===\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "ap = average_precision_score(y_test, y_scores)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f\"AP = {ap:.3f}\", color=\"purple\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(f\"Precision–Recall Curve — {model_name}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uI6ep_cJ6k0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGBM"
      ],
      "metadata": {
        "id": "ddKsV2UQiAAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Final test‑set evaluation for a single tuned model\n",
        "# ------------------------------------------------------------\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose the model you want to evaluate\n",
        "model_name = \"LightGBM\"                 # <-- change to \"XGBoost\", \"RandomForest\", ...\n",
        "model      = best_models[model_name]    # already refit by GridSearchCV\n",
        "\n",
        "# Predict class labels and fraud probabilities on the test set\n",
        "y_pred   = model.predict(X_test)\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Print standard metrics\n",
        "print(f\"\\n=== {model_name} — Test‑set performance ===\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "# Precision–Recall curve and Average Precision (AUPRC)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "ap = average_precision_score(y_test, y_scores)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f\"AP = {ap:.3f}\", color=\"purple\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(f\"Precision–Recall Curve — {model_name}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZuzVDtkJh-if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balanced Random Forest\n"
      ],
      "metadata": {
        "id": "XCJzmRzW6pGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Final Test Evaluation: Balanced Random Forest ===\n",
        "model_name = \"Balanced Random Forest\"\n",
        "model = best_models[model_name]\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(f\"\\n=== {model_name} — Test-set performance ===\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "ap = average_precision_score(y_test, y_scores)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f\"AP = {ap:.3f}\", color=\"purple\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(f\"Precision–Recall Curve — {model_name}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uzOFiv1_6q1n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZKMa7oq8vsL8bJJtK1p2qBgKnJCmWf63",
      "authorship_tag": "ABX9TyMEsgRs3rg+DOfyNoSrGQ9i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}